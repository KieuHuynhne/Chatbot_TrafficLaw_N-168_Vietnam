from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
import sentencepiece


# T·∫°o ·ª©ng d·ª•ng FastAPI
app = FastAPI()

# C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ c√°c ngu·ªìn, b·∫°n c√≥ th·ªÉ ch·ªâ ƒë·ªãnh ["http://127.0.0.1:5500"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ƒê∆∞·ªùng d·∫´n t·ªõi c√°c m√¥ h√¨nh
models = {
    "t5": "models/vit5_finetuned_QA",
    "gpt2": "models/gpt2_finetuned_vi",
    "bartpho": "models/bartpho-qa"
}
from transformers import AutoTokenizer

# T·∫£i tokenizer n·∫øu ch∆∞a c√≥ trong checkpoint Bartpho
import os
bartpho_checkpoint = "models/bartpho-qa/checkpoint-1194"
if not os.path.exists(os.path.join(bartpho_checkpoint, "tokenizer.json")):
    print("üîπ Kh√¥ng t√¨m th·∫•y tokenizer cho Bartpho, ƒëang t·∫£i l·∫°i t·ª´ Hugging Face...")
    tokenizer = AutoTokenizer.from_pretrained("vinai/bartpho-word")
    tokenizer.save_pretrained(bartpho_checkpoint)
    print("‚úÖ ƒê√£ t·∫£i v√† l∆∞u tokenizer v√†o checkpoint")


# Cache c√°c pipeline ƒë√£ t·∫£i
loaded_pipelines = {}

def load_pipeline(model_name):
    """
    H√†m t·∫£i pipeline cho m√¥ h√¨nh t∆∞∆°ng ·ª©ng.
    """
    if model_name not in loaded_pipelines:
        if model_name not in models:
            raise ValueError(f"Model {model_name} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
        
        tokenizer = AutoTokenizer.from_pretrained(models[model_name])

        # Ki·ªÉm tra lo·∫°i m√¥ h√¨nh ƒë·ªÉ t·∫£i ƒë√∫ng class
        if model_name == "t5" or model_name == "bartpho":  # M√¥ h√¨nh Seq2Seq nh∆∞ T5
            model = AutoModelForSeq2SeqLM.from_pretrained(models[model_name])
            loaded_pipelines[model_name] = pipeline(
                "text2text-generation",
                model=model,
                tokenizer=tokenizer,
                max_length=128,
                num_beams=5,
                early_stopping=True,
            )
        elif model_name == "gpt2":  # M√¥ h√¨nh causal nh∆∞ GPT-2
            model = AutoModelForCausalLM.from_pretrained(models[model_name])
            loaded_pipelines[model_name] = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_length=128,
                num_return_sequences=1,
            )
        else:
            raise ValueError(f"Model {model_name} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
    
    return loaded_pipelines[model_name]

# ƒê·ªãnh nghƒ©a input schema
class QuestionRequest(BaseModel):
    question: str
    model: str  

@app.post("/ask")
async def answer_question(request: QuestionRequest):
    """
    Endpoint nh·∫≠n c√¢u h·ªèi v√† m√¥ h√¨nh ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi.
    """
    # Load pipeline cho m√¥ h√¨nh ƒë∆∞·ª£c ch·ªçn
    qa_pipeline = load_pipeline(request.model)
    
    if request.model == "gpt2":
        # GPT-2 sinh output kh√°c v·ªõi Seq2Seq
        output = qa_pipeline(request.question, max_length=128, num_return_sequences=1)
        answer = output[0]["generated_text"]
    else:
        # T5 ho·∫∑c m√¥ h√¨nh Seq2Seq kh√°c
        output = qa_pipeline(request.question)
        answer = output[0]["generated_text"]
    
    return {"answer": answer}